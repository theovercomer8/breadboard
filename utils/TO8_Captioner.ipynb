{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Install Dependencies\n",
        "#@markdown Installs requirements.txt and imports CLIP data\n",
        "import os\n",
        "\n",
        "if os.path.exists('/content/data'):\n",
        "  !rm -rf /content/data\n",
        "\n",
        "\n",
        "if os.path.exists('/content/requirements.txt'):\n",
        "  !rm /content/requirements.txt\n",
        "\n",
        "!mkdir /content/data\n",
        "%cd /content/data\n",
        "\n",
        "\n",
        "!wget https://raw.githubusercontent.com/theovercomer8/breadboard/main/utils/data/artists.txt\n",
        "!wget https://raw.githubusercontent.com/theovercomer8/breadboard/main/utils/data/flavors.txt\n",
        "!wget https://raw.githubusercontent.com/theovercomer8/breadboard/main/utils/data/movements.txt\n",
        "!wget https://raw.githubusercontent.com/theovercomer8/breadboard/main/utils/data/mediums.txt\n",
        "\n",
        "%cd /content\n",
        "!wget https://raw.githubusercontent.com/theovercomer8/breadboard/main/requirements.txt\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "\n",
        "if not os.path.exists('/content/dataset'):\n",
        "  !mkdir /content/dataset"
      ],
      "metadata": {
        "id": "ie6oPstv4Xx5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnmjTJVy0aNL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4d0644-97b0-421b-d0f4-16b6a7d080ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BLIP model...\n"
          ]
        }
      ],
      "source": [
        "#@title Caption Wizard\n",
        "\n",
        "from json import JSONDecoder, JSONEncoder\n",
        "import sys\n",
        "import getopt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "import hashlib\n",
        "import inspect\n",
        "import math\n",
        "import numpy as np\n",
        "import open_clip\n",
        "import pickle\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from blip.models.blip import blip_decoder, BLIP_Decoder\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "\n",
        "\n",
        "# All objects we find\n",
        "json_found = []  \n",
        "# raw_decode expects byte1 to be part of a JSON, so remove whitespace from left\n",
        "# stdin = sys.stdin.read().lstrip()\n",
        "decoder = JSONDecoder()\n",
        "encoder = JSONEncoder()\n",
        "\n",
        "folder_path = '/content/dataset' #@param {type:\"string\"}\n",
        "git_fail_phrases = 'a sign that says,writing that says,that says' #@param {type:\"string\"}\n",
        "git_pass = True #@param {type:\"boolean\"}\n",
        "blip_pass = True #@param {type:\"boolean\"}\n",
        "cap_length = 75 #@param {type: \"slider\", min: 0, max: 400}\n",
        "existing = 'ignore' #@param [ 'ignore', 'copy', 'prepend', 'append']\n",
        "clip_beams = 8 #@param {type: \"slider\", min: 1, max: 20}\n",
        "clip_min = 30 #@param {type: \"slider\", min: 5, max: 75}\n",
        "clip_max = 50 #@param {type: \"slider\", min: 5, max: 75}\n",
        "clip_v2 = True #@param {type:\"boolean\"}\n",
        "clip_use_flavor = True #@param {type:\"boolean\"}\n",
        "clip_max_flavors = 4 #@param {type: \"slider\", min: 1, max: 10}\n",
        "clip_use_artist = False #@param {type:\"boolean\"}\n",
        "clip_use_medium = False #@param {type:\"boolean\"}\n",
        "clip_use_movement = False #@param {type:\"boolean\"}\n",
        "clip_use_trending = False  #@param {type:\"boolean\"}\n",
        "ignore_tags = '' #@param {type:\"string\"}\n",
        "replace_class = False #@param {type:\"boolean\"}\n",
        "sub_class = '' #@param {type:\"string\"}\n",
        "sub_name = '' #@param {type:\"string\"}\n",
        "folder_tag = False #@param {type:\"boolean\"}\n",
        "folder_tag_levels = 1 #@param {type: \"slider\", min: 1, max: 10}\n",
        "uniquify_tags = True #@param {type:\"boolean\"}\n",
        "write_to_file = True #@param {type:\"boolean\"}\n",
        "use_filename = False #@param {type:\"boolean\"}\n",
        "\n",
        "device = \"cuda\" #if torch.cuda.is_available() else \"cpu\"\n",
        "processor = None\n",
        "model = None\n",
        "\n",
        "def get_parent_folder(filepath, levels=1):\n",
        "    common = os.path.split(filepath)[0]\n",
        "    paths = []\n",
        "    for i in range(int(levels)):\n",
        "        split = os.path.split(common)\n",
        "        common = split[0]\n",
        "        paths.append(split[1])\n",
        "    return paths\n",
        "\n",
        "\n",
        "def git_caption(img):\n",
        "    pixel_values = processor(images=img, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "    pixel_values = pixel_values.to(device)\n",
        "    generated_ids = model.generate(pixel_values=pixel_values, max_length=150)\n",
        "    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return generated_caption\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "BLIP_MODELS = {\n",
        "    'base': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth',\n",
        "    'large': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth'\n",
        "}\n",
        "\n",
        "@dataclass \n",
        "class Config:\n",
        "    # models can optionally be passed in directly\n",
        "    blip_model: BLIP_Decoder = None\n",
        "    clip_model = None\n",
        "    clip_preprocess = None\n",
        "\n",
        "    # blip settings\n",
        "    blip_image_eval_size: int = 384\n",
        "    blip_model_type: str = 'large' # choose between 'base' or 'large'\n",
        "    blip_offload: bool = False\n",
        "\n",
        "    # clip settings\n",
        "    clip_model_name: str = 'ViT-L-14/openai' if not clip_v2 else 'coca_ViT-L-14/mscoco_finetuned_laion2B-s13B-b90k'\n",
        "    clip_model_path: str = None\n",
        "\n",
        "    # interrogator settings\n",
        "    cache_path: str = 'cache'\n",
        "    chunk_size: int = 2048\n",
        "    data_path: str = '/content/data'\n",
        "    device: str = (\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    flavor_intermediate_count: int = 2048\n",
        "    quiet: bool = False # when quiet progress bars are not shown\n",
        "\n",
        "class Interrogator():\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        if blip_pass:\n",
        "            if config.blip_model is None:\n",
        "                if not config.quiet:\n",
        "                    print(\"Loading BLIP model...\")\n",
        "                blip_path = os.path.dirname(inspect.getfile(blip_decoder))\n",
        "                configs_path = os.path.join(os.path.dirname(blip_path), 'configs')\n",
        "                med_config = os.path.join(configs_path, 'med_config.json')\n",
        "                blip_model = blip_decoder(\n",
        "                    pretrained=BLIP_MODELS[config.blip_model_type],\n",
        "                    image_size=config.blip_image_eval_size, \n",
        "                    vit=config.blip_model_type, \n",
        "                    med_config=med_config\n",
        "                )\n",
        "                blip_model.eval()\n",
        "                blip_model = blip_model.to(config.device)\n",
        "                self.blip_model = blip_model\n",
        "            else:\n",
        "                self.blip_model = config.blip_model\n",
        "\n",
        "        if clip_use_movement or clip_use_artist or clip_use_flavor or clip_use_medium or clip_use_trending:\n",
        "            self.load_clip_model()\n",
        "\n",
        "    def load_clip_model(self):\n",
        "        start_time = time.time()\n",
        "        config = self.config\n",
        "\n",
        "        if config.clip_model is None:\n",
        "            if not config.quiet:\n",
        "                print(\"Loading CLIP model...\")\n",
        "\n",
        "            clip_model_name, clip_model_pretrained_name = config.clip_model_name.split('/', 2)\n",
        "            self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms(\n",
        "                clip_model_name, \n",
        "                pretrained=clip_model_pretrained_name, \n",
        "                precision='fp16' if config.device == 'cuda' else 'fp32',\n",
        "                device=config.device,\n",
        "                jit=False,\n",
        "                cache_dir=config.clip_model_path\n",
        "            )\n",
        "            self.clip_model.to(config.device).eval()\n",
        "        else:\n",
        "            self.clip_model = config.clip_model\n",
        "            self.clip_preprocess = config.clip_preprocess\n",
        "        self.tokenize = open_clip.get_tokenizer(clip_model_name)\n",
        "\n",
        "        sites = ['Artstation', 'behance', 'cg society', 'cgsociety', 'deviantart', 'dribble', 'flickr', 'instagram', 'pexels', 'pinterest', 'pixabay', 'pixiv', 'polycount', 'reddit', 'shutterstock', 'tumblr', 'unsplash', 'zbrush central']\n",
        "        trending_list = [site for site in sites]\n",
        "        trending_list.extend([\"trending on \"+site for site in sites])\n",
        "        trending_list.extend([\"featured on \"+site for site in sites])\n",
        "        trending_list.extend([site+\" contest winner\" for site in sites])\n",
        "\n",
        "        raw_artists = _load_list(config.data_path, 'artists.txt')\n",
        "        artists = [f\"by {a}\" for a in raw_artists]\n",
        "        artists.extend([f\"inspired by {a}\" for a in raw_artists])\n",
        "\n",
        "        if clip_use_artist:\n",
        "            self.artists = LabelTable(artists, \"artists\", self.clip_model, self.tokenize, config)\n",
        "        \n",
        "        if clip_use_flavor:\n",
        "            self.flavors = LabelTable(_load_list(config.data_path, 'flavors.txt'), \"flavors\", self.clip_model, self.tokenize, config)\n",
        "        \n",
        "        if clip_use_medium:\n",
        "            self.mediums = LabelTable(_load_list(config.data_path, 'mediums.txt'), \"mediums\", self.clip_model, self.tokenize, config)\n",
        "        \n",
        "        if clip_use_movement:\n",
        "            self.movements = LabelTable(_load_list(config.data_path, 'movements.txt'), \"movements\", self.clip_model, self.tokenize, config)\n",
        "        \n",
        "        if clip_use_trending:\n",
        "            self.trendings = LabelTable(trending_list, \"trendings\", self.clip_model, self.tokenize, config)\n",
        "\n",
        "        end_time = time.time()\n",
        "        if not config.quiet:\n",
        "            print(f\"Loaded CLIP model and data in {end_time-start_time:.2f} seconds.\")\n",
        "\n",
        "    def generate_blip_caption(self, pil_image: Image) -> str:\n",
        "        if self.config.blip_offload:\n",
        "            self.blip_model = self.blip_model.to(self.device)\n",
        "        size = self.config.blip_image_eval_size\n",
        "        gpu_image = transforms.Compose([\n",
        "            transforms.Resize((size, size), interpolation=InterpolationMode.BICUBIC),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ])(pil_image).unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            caption = self.blip_model.generate(\n",
        "                gpu_image, \n",
        "                sample=False, \n",
        "                num_beams=clip_beams, \n",
        "                max_length=clip_max, \n",
        "                min_length=clip_min\n",
        "            )\n",
        "        if self.config.blip_offload:\n",
        "            self.blip_model = self.blip_model.to(\"cpu\")\n",
        "        return caption[0]\n",
        "\n",
        "    def image_to_features(self, image: Image) -> torch.Tensor:\n",
        "        images = self.clip_preprocess(image).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            image_features = self.clip_model.encode_image(images)\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        return image_features\n",
        "\n",
        "    \n",
        "    def interrogate(self, caption: str, image: Image) -> str:\n",
        "        image_features = self.image_to_features(image)\n",
        "\n",
        "        # flaves = self.flavors.rank(image_features, self.config.flavor_intermediate_count)\n",
        "        # best_medium = self.mediums.rank(image_features, 1)[0]\n",
        "        # best_artist = self.artists.rank(image_features, 1)[0]\n",
        "        # best_trending = self.trendings.rank(image_features, 1)[0]\n",
        "        # best_movement = self.movements.rank(image_features, 1)[0]\n",
        "\n",
        "        best_prompt = caption\n",
        "        best_sim = self.similarity(image_features, best_prompt)\n",
        "\n",
        "        def check(addition: str) -> bool:\n",
        "            nonlocal best_prompt, best_sim\n",
        "            prompt = best_prompt + \", \" + addition\n",
        "            sim = self.similarity(image_features, prompt)\n",
        "            if sim > best_sim:\n",
        "                best_sim = sim\n",
        "                best_prompt = prompt\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "        def check_multi_batch(opts: List[str]):\n",
        "            nonlocal best_prompt, best_sim\n",
        "            prompts = []\n",
        "            for i in range(2**len(opts)):\n",
        "                prompt = best_prompt\n",
        "                for bit in range(len(opts)):\n",
        "                    if i & (1 << bit):\n",
        "                        prompt += \", \" + opts[bit]\n",
        "                prompts.append(prompt)\n",
        "\n",
        "            t = LabelTable(prompts, None, self.clip_model, self.tokenize, self.config)\n",
        "            best_prompt = t.rank(image_features, 1)[0]\n",
        "            best_sim = self.similarity(image_features, best_prompt)\n",
        "\n",
        "        batch = []\n",
        "\n",
        "        if clip_use_artist:\n",
        "            batch.append(self.artists.rank(image_features,1)[0])\n",
        "        if clip_use_flavor:\n",
        "                best_flavors = self.flavors.rank(image_features, self.config.flavor_intermediate_count)\n",
        "                extended_flavors = set(best_flavors)\n",
        "                for _ in tqdm(range(clip_max_flavors), desc=\"Flavor chain\", disable=self.config.quiet):\n",
        "                    best = self.rank_top(image_features, [f\"{best_prompt}, {f}\" for f in extended_flavors])\n",
        "                    flave = best[len(best_prompt) + 2:]\n",
        "                    if not check(flave):\n",
        "                        break\n",
        "                    if _prompt_at_max_len(best_prompt, self.tokenize):\n",
        "                        break\n",
        "                    extended_flavors.remove(flave)\n",
        "        if clip_use_medium:\n",
        "            batch.append(self.mediums.rank(image_features, 1)[0])\n",
        "        if clip_use_trending:\n",
        "            batch.append(self.trendings.rank(image_features, 1)[0])\n",
        "        if clip_use_movement:\n",
        "            batch.append(self.movements.rank(image_features, 1)[0])\n",
        "\n",
        "        check_multi_batch(batch)\n",
        "        tags = best_prompt.split(\",\")\n",
        "\n",
        "        return tags\n",
        "        # check_multi_batch([best_medium, best_artist, best_trending, best_movement])\n",
        "\n",
        "        # extended_flavors = set(flaves)\n",
        "        # for _ in tqdm(range(max_flavors), desc=\"Flavor chain\", disable=self.config.quiet):\n",
        "        #     best = self.rank_top(image_features, [f\"{best_prompt}, {f}\" for f in extended_flavors])\n",
        "        #     flave = best[len(best_prompt)+2:]\n",
        "        #     if not check(flave):\n",
        "        #         break\n",
        "        #     if _prompt_at_max_len(best_prompt, self.tokenize):\n",
        "        #         break\n",
        "        #     extended_flavors.remove(flave)\n",
        "\n",
        "        # return best_prompt\n",
        "\n",
        "    def rank_top(self, image_features: torch.Tensor, text_array: List[str]) -> str:\n",
        "        text_tokens = self.tokenize([text for text in text_array]).to(self.device)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            text_features = self.clip_model.encode_text(text_tokens)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = text_features @ image_features.T\n",
        "        return text_array[similarity.argmax().item()]\n",
        "\n",
        "    def similarity(self, image_features: torch.Tensor, text: str) -> float:\n",
        "        text_tokens = self.tokenize([text]).to(self.device)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            text_features = self.clip_model.encode_text(text_tokens)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = text_features @ image_features.T\n",
        "        return similarity[0][0].item()\n",
        "\n",
        "\n",
        "class LabelTable():\n",
        "    def __init__(self, labels:List[str], desc:str, clip_model, tokenize, config: Config):\n",
        "        self.chunk_size = config.chunk_size\n",
        "        self.config = config\n",
        "        self.device = config.device\n",
        "        self.embeds = []\n",
        "        self.labels = labels\n",
        "        self.tokenize = tokenize\n",
        "\n",
        "        hash = hashlib.sha256(\",\".join(labels).encode()).hexdigest()\n",
        "\n",
        "        cache_filepath = None\n",
        "        if config.cache_path is not None and desc is not None:\n",
        "            os.makedirs(config.cache_path, exist_ok=True)\n",
        "            sanitized_name = config.clip_model_name.replace('/', '_').replace('@', '_')\n",
        "            cache_filepath = os.path.join(config.cache_path, f\"{sanitized_name}_{desc}.pkl\")\n",
        "            if desc is not None and os.path.exists(cache_filepath):\n",
        "                with open(cache_filepath, 'rb') as f:\n",
        "                    try:\n",
        "                        data = pickle.load(f)\n",
        "                        if data.get('hash') == hash:\n",
        "                            self.labels = data['labels']\n",
        "                            self.embeds = data['embeds']\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading cached table {desc}: {e}\")\n",
        "\n",
        "        if len(self.labels) != len(self.embeds):\n",
        "            self.embeds = []\n",
        "            chunks = np.array_split(self.labels, max(1, len(self.labels)/config.chunk_size))\n",
        "            for chunk in tqdm(chunks, desc=f\"Preprocessing {desc}\" if desc else None, disable=self.config.quiet):\n",
        "                text_tokens = self.tokenize(chunk).to(self.device)\n",
        "                with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "                    text_features = clip_model.encode_text(text_tokens)\n",
        "                    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "                    text_features = text_features.half().cpu().numpy()\n",
        "                for i in range(text_features.shape[0]):\n",
        "                    self.embeds.append(text_features[i])\n",
        "\n",
        "            if cache_filepath is not None:\n",
        "                with open(cache_filepath, 'wb') as f:\n",
        "                    pickle.dump({\n",
        "                        \"labels\": self.labels, \n",
        "                        \"embeds\": self.embeds, \n",
        "                        \"hash\": hash, \n",
        "                        \"model\": config.clip_model_name\n",
        "                    }, f)\n",
        "\n",
        "        if self.device == 'cpu' or self.device == torch.device('cpu'):\n",
        "            self.embeds = [e.astype(np.float32) for e in self.embeds]\n",
        "    \n",
        "    def _rank(self, image_features: torch.Tensor, text_embeds: torch.Tensor, top_count: int=1) -> str:\n",
        "        top_count = min(top_count, len(text_embeds))\n",
        "        text_embeds = torch.stack([torch.from_numpy(t) for t in text_embeds]).to(self.device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            similarity = image_features @ text_embeds.T\n",
        "        _, top_labels = similarity.float().cpu().topk(top_count, dim=-1)\n",
        "        return [top_labels[0][i].numpy() for i in range(top_count)]\n",
        "\n",
        "    def rank(self, image_features: torch.Tensor, top_count: int=1) -> List[str]:\n",
        "        if len(self.labels) <= self.chunk_size:\n",
        "            tops = self._rank(image_features, self.embeds, top_count=top_count)\n",
        "            return [self.labels[i] for i in tops]\n",
        "\n",
        "        num_chunks = int(math.ceil(len(self.labels)/self.chunk_size))\n",
        "        keep_per_chunk = int(self.chunk_size / num_chunks)\n",
        "\n",
        "        top_labels, top_embeds = [], []\n",
        "        for chunk_idx in tqdm(range(num_chunks), disable=self.config.quiet):\n",
        "            start = chunk_idx*self.chunk_size\n",
        "            stop = min(start+self.chunk_size, len(self.embeds))\n",
        "            tops = self._rank(image_features, self.embeds[start:stop], top_count=keep_per_chunk)\n",
        "            top_labels.extend([self.labels[start+i] for i in tops])\n",
        "            top_embeds.extend([self.embeds[start+i] for i in tops])\n",
        "\n",
        "        tops = self._rank(image_features, top_embeds, top_count=top_count)\n",
        "        return [top_labels[i] for i in tops]\n",
        "\n",
        "\n",
        "def _load_list(data_path: str, filename: str) -> List[str]:\n",
        "    with open(os.path.join(data_path, filename), 'r', encoding='utf-8', errors='replace') as f:\n",
        "        items = [line.strip() for line in f.readlines()]\n",
        "    return items\n",
        "\n",
        "def _merge_tables(tables: List[LabelTable], config: Config) -> LabelTable:\n",
        "    m = LabelTable([], None, None, None, config)\n",
        "    for table in tables:\n",
        "        m.labels.extend(table.labels)\n",
        "        m.embeds.extend(table.embeds)\n",
        "    return m\n",
        "\n",
        "def _prompt_at_max_len(text: str, tokenize) -> bool:\n",
        "    tokens = tokenize([text])\n",
        "    return tokens[0][-1] != 0\n",
        "\n",
        "def _truncate_to_fit(text: str, tokenize) -> str:\n",
        "    parts = text.split(', ')\n",
        "    new_text = parts[0]\n",
        "    for part in parts[1:]:\n",
        "        if _prompt_at_max_len(new_text + part, tokenize):\n",
        "            break\n",
        "        new_text += ', ' + part\n",
        "    return new_text\n",
        "\n",
        "ci:Interrogator = None\n",
        "\n",
        "        \n",
        "def process_img(img_path):\n",
        "    # Load image\n",
        "    img = Image.open(img_path).convert('RGB')\n",
        "\n",
        "    # Get existing caption\n",
        "    existing_caption = ''\n",
        "    cap_file = os.path.splitext(os.path.split(img_path)[1])[0] + '.txt'\n",
        "    if os.path.isfile(cap_file):\n",
        "      with open(cap_file) as f:\n",
        "        existing_caption = f.read()\n",
        "\n",
        "    # Get caption from filename if empty\n",
        "    if existing_caption == '' and use_filename:\n",
        "        path = os.path.split(img_path)[1]\n",
        "        path = os.path.splitext(path)[0]\n",
        "        existing_caption = ''.join(c for c in path if c.isalpha() or c in [\" \", \",\"])\n",
        "    \n",
        "    \n",
        "    # Create tag list\n",
        "    out_tags = []\n",
        "    new_caption = ''\n",
        "   \n",
        "    # 1st caption pass: GIT\n",
        "    if git_pass:\n",
        "        new_caption = git_caption(img)\n",
        "        print('Got git caption: ',new_caption)\n",
        "        # Check if caption fails from list of not-allowed phrases\n",
        "        if blip_pass and any(f in new_caption for f in git_fail_phrases.split(',')):\n",
        "            # Fail git caption\n",
        "            new_caption = ''\n",
        "\n",
        "    # 2nd caption pass: BLIP (if failed)\n",
        "    if blip_pass and new_caption == '':\n",
        "        new_caption = ci.generate_blip_caption(img)\n",
        "\n",
        "\n",
        "\n",
        "    # Add enabled CLIP flavors to tag list\n",
        "    if clip_use_artist or clip_use_flavor or clip_use_medium or clip_use_movement or clip_use_trending:\n",
        "        tags = ci.interrogate(new_caption,img)\n",
        "        for tag in tags:\n",
        "            out_tags.append(tag)\n",
        "    else:\n",
        "        for tag in new_caption.split(\",\"):\n",
        "            out_tags.append(tag)\n",
        "\n",
        "\n",
        "    # Add parent folder to tag list if enabled\n",
        "    if folder_tag:\n",
        "        folder_tags = get_parent_folder(img_path,folder_tag_levels)\n",
        "        for tag in folder_tags:\n",
        "            out_tags.append(tag)\n",
        "\n",
        "    # Remove duplicates, filter dumb stuff\n",
        "    # chars_to_strip = [\"_\\\\(\"]\n",
        "    unique_tags = []\n",
        "    tags_to_ignore = []\n",
        "    if ignore_tags != \"\" and ignore_tags is not None:\n",
        "        si_tags = ignore_tags.split(\",\")\n",
        "        for tag in si_tags:\n",
        "            tags_to_ignore.append(tag.strip)\n",
        "\n",
        "    if uniquify_tags:\n",
        "        for tag in out_tags:\n",
        "            if not tag in unique_tags and not \"_\\(\" in tag and not tag in ignore_tags:\n",
        "                unique_tags.append(tag.strip())\n",
        "    else:\n",
        "         for tag in out_tags:\n",
        "            if not \"_\\(\" in tag and not tag in ignore_tags:\n",
        "                unique_tags.append(tag.strip())\n",
        "\n",
        "    existing_tags = existing_caption.split(\",\")\n",
        "\n",
        "    # APPEND/PREPEND/OVERWRITE existing caption based on options\n",
        "    if existing == \"prepend\" and len(existing_tags):\n",
        "        new_tags = existing_tags\n",
        "        for tag in unique_tags:\n",
        "            if not tag in new_tags or not uniquify_tags:\n",
        "                new_tags.append(tag)\n",
        "        unique_tags = new_tags\n",
        "\n",
        "    if existing == 'append' and len(existing_tags):\n",
        "        for tag in existing_tags:\n",
        "            if not tag in unique_tags or not uniquify_tags:\n",
        "                unique_tags.append(tag)\n",
        "\n",
        "    if existing == 'copy' and existing_caption:\n",
        "        for tag in existing_tags:\n",
        "            unique_tags.append(tag.strip())\n",
        "\n",
        "    # Construct new caption from tag list\n",
        "    caption_txt = \", \".join(unique_tags)\n",
        "\n",
        "    if replace_class and sub_name is not None and sub_class is not None:\n",
        "        # Find and replace \"a SUBJECT CLASS\" in caption_txt with subject name\n",
        "        if f\"a {sub_class}\" in caption_txt:\n",
        "            caption_txt = caption_txt.replace(f\"a {sub_class}\", sub_name)\n",
        "\n",
        "        if sub_class in caption_txt:\n",
        "            caption_txt = caption_txt.replace(sub_class, sub_name)\n",
        "\n",
        "    tags = caption_txt.split(\" \")\n",
        "    if cap_length != 0 and len(tags) > cap_length:\n",
        "            tags = tags[0:cap_length]\n",
        "            tags[-1] = tags[-1].rstrip(\",\")\n",
        "    caption_txt = \" \".join(tags)\n",
        "\n",
        "    # Write caption file\n",
        "    if write_to_file:\n",
        "        with open(cap_file, \"w\", encoding=\"utf8\") as file:\n",
        "                    file.write(caption_txt)\n",
        "                    print(f'Wrote {cap_file}')\n",
        "\n",
        "    # Return caption text to Breadboard\n",
        "    print('PROCESSED: {0} | NEW CAPTION:{1}'.format(img_path,caption_txt))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### INIT HERE. SEND --READY-- WHEN READY FOR INPUT\n",
        "if git_pass:\n",
        "    processor = AutoProcessor.from_pretrained(\"microsoft/git-large-r-textcaps\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-large-r-textcaps\")\n",
        "    model.to(device)\n",
        "\n",
        "if clip_use_movement or clip_use_artist or clip_use_flavor or clip_use_medium or clip_use_trending or blip_pass:\n",
        "    ci = Interrogator(Config(clip_model_name='ViT-L-14/openai' if not clip_v2 else 'coca_ViT-L-14/mscoco_finetuned_laion2B-s13B-b90k',\n",
        "                          quiet=False))\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(folder_path, topdown=False):\n",
        "   for name in files:\n",
        "     if 'txt' not in os.path.splitext(os.path.split(name)[1])[1]:\n",
        "      process_img(os.path.join(root, name))\n",
        "   \n",
        "   "
      ]
    }
  ]
}